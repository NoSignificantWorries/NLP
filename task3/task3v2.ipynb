{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0ea21c6190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
    "DATASET_NAME = \"Davlan/sib200\"\n",
    "DATASET_LANGUAGE = \"rus_Cyrl\"\n",
    "MINIBATCH_SIZE = 8\n",
    "MAX_LEN = 512\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель та же, но уменьшил размер минибатча, дало прирост итоговой метрики, и аналогично ограничил максимальную длину. Также простенькая нормализация для стабильности.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    import unicodedata\n",
    "\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_batch_texts(texts):\n",
    "    return [normalize_text(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовые быстрые и простые аугментации чтобы увеличить количество примеров на слабо представленные классы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    parts = re.split(r\"([.!?])\", text)\n",
    "    sents = []\n",
    "    for i in range(0, len(parts), 2):\n",
    "        seg = parts[i].strip()\n",
    "        if not seg:\n",
    "            continue\n",
    "        end = parts[i + 1] if i + 1 < len(parts) else \"\"\n",
    "        sents.append((seg + end).strip())\n",
    "    return [s for s in sents if s]\n",
    "\n",
    "\n",
    "def aug_sentence_shuffle(text, p=0.35):\n",
    "    if np.random.rand() > p:\n",
    "        return text\n",
    "    sents = split_sentences(text)\n",
    "    if len(sents) < 2:\n",
    "        return text\n",
    "    np.random.shuffle(sents)\n",
    "    return \" \".join(sents)\n",
    "\n",
    "\n",
    "def aug_punct_swap(text, p=0.25):\n",
    "    if np.random.rand() > p:\n",
    "        return text\n",
    "    puncts = [\".\", \",\", \"!\", \"?\"]\n",
    "    chars = list(text)\n",
    "    for i, ch in enumerate(chars):\n",
    "        if ch in puncts and np.random.rand() < 0.2:\n",
    "            chars[i] = np.random.choice(puncts)\n",
    "    return \"\".join(chars)\n",
    "\n",
    "\n",
    "def aug_truncate_middle(text, p=0.25):\n",
    "    if np.random.rand() > p:\n",
    "        return text\n",
    "    words = text.split()\n",
    "    n = len(words)\n",
    "    if n < 12:\n",
    "        return text\n",
    "    cut = int(n * np.random.uniform(0.1, 0.3))\n",
    "    start_keep = int((n - cut) / 2)\n",
    "    new_words = words[:start_keep] + words[start_keep + cut :]\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "def apply_augmentations(text):\n",
    "    text = aug_sentence_shuffle(text, p=0.4)\n",
    "    text = aug_punct_swap(text, p=0.3)\n",
    "    text = aug_truncate_middle(text, p=0.3)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_dataset(DATASET_NAME, DATASET_LANGUAGE, split=\"train\")\n",
    "validation_set = load_dataset(DATASET_NAME, DATASET_LANGUAGE, split=\"validation\")\n",
    "test_set = load_dataset(DATASET_NAME, DATASET_LANGUAGE, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normilized text example: Турция с трёх сторон окружена морями: на западе — Эгейским, на севере — Чёрным и на юге — Средиземны...\n"
     ]
    }
   ],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    normalized_texts = normalize_batch_texts(dataset[\"text\"])\n",
    "    return dataset.remove_columns([\"text\"]).add_column(\"text\", normalized_texts)\n",
    "\n",
    "\n",
    "train_set = normalize_dataset(train_set)\n",
    "validation_set = normalize_dataset(validation_set)\n",
    "test_set = normalize_dataset(test_set)\n",
    "\n",
    "print(f\"Normilized text example: {train_set[0]['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classes: 7\n",
      "Examples: 701\n",
      "\n",
      "By class:\n",
      "  science/technology: 176 examples (25.11%)\n",
      "  travel: 138 examples (19.69%)\n",
      "  politics: 102 examples (14.55%)\n",
      "  sports: 85 examples (12.13%)\n",
      "  health: 77 examples (10.98%)\n",
      "  entertainment: 65 examples (9.27%)\n",
      "  geography: 58 examples (8.27%)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_set[\"category\"]\n",
    "label_counts = collections.Counter(train_labels)\n",
    "\n",
    "print(f\"All classes: {len(label_counts)}\")\n",
    "print(f\"Examples: {len(train_labels)}\")\n",
    "print(\"\\nBy class:\")\n",
    "for label, count in label_counts.most_common():\n",
    "    print(f\"  {label}: {count} examples ({count / len(train_labels) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding augmented examples: 211 (target level ~167)\n"
     ]
    }
   ],
   "source": [
    "counts = collections.Counter(train_set[\"category\"])\n",
    "rare_classes = {\"entertainment\", \"geography\"}\n",
    "if len(counts) > 0:\n",
    "    target = int(max(counts.values()) * 0.95)\n",
    "    aug_texts, aug_labels = [], []\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    for label in counts:\n",
    "        if label not in rare_classes:\n",
    "            continue\n",
    "        need = max(0, target - counts[label])\n",
    "        if need == 0:\n",
    "            continue\n",
    "        idxs = [i for i, l in enumerate(train_set[\"category\"]) if l == label]\n",
    "        for _ in range(need):\n",
    "            i = int(rng.choice(idxs))\n",
    "            base_text = train_set[i][\"text\"]\n",
    "            new_text = apply_augmentations(base_text)\n",
    "            aug_texts.append(new_text)\n",
    "            aug_labels.append(label)\n",
    "\n",
    "    if aug_texts:\n",
    "        print(f\"Adding augmented examples: {len(aug_texts)} (target level ~{target})\")\n",
    "        aug_ds = Dataset.from_dict({\"text\": aug_texts, \"category\": aug_labels})\n",
    "        train_set = concatenate_datasets([train_set, aug_ds])\n",
    "        train_set = train_set.shuffle(seed=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "USE_PSEUDO_LABELING = True\n",
    "PSEUDO_FRACTION = 0.2\n",
    "PSEUDO_MIN_PROB = 0.80\n",
    "\n",
    "if USE_PSEUDO_LABELING:\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(set(train_set[\"category\"])),\n",
    "        classifier_dropout=0.1,\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "    )\n",
    "\n",
    "    tmp_tokenized = train_set.map(\n",
    "        lambda batch: tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN),\n",
    "        batched=True,\n",
    "    )\n",
    "    tmp_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    base_model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    base_model.to(device)\n",
    "\n",
    "    pseudo_texts = []\n",
    "    pseudo_labels = []\n",
    "\n",
    "    n_pseudo_candidates = int(len(tmp_tokenized) * PSEUDO_FRACTION)\n",
    "    idxs_for_pseudo = np.random.choice(\n",
    "        len(tmp_tokenized), size=n_pseudo_candidates, replace=False\n",
    "    )\n",
    "\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    with torch.no_grad():\n",
    "        for idx in idxs_for_pseudo:\n",
    "            sample = tmp_tokenized[idx]\n",
    "            input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "            attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "            outputs = base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = softmax(logits)[0].cpu().numpy()\n",
    "            pred_idx = int(np.argmax(probs))\n",
    "            conf = float(probs[pred_idx])\n",
    "\n",
    "            if conf >= PSEUDO_MIN_PROB:\n",
    "                label = base_model.config.id2label.get(pred_idx, None)\n",
    "                if label == \"entertainment\":\n",
    "                    pseudo_texts.append(train_set[idx][\"text\"])\n",
    "                    pseudo_labels.append(label)\n",
    "\n",
    "    if pseudo_texts:\n",
    "        print(f\"Pseudolabled class 'entertainment': {len(pseudo_texts)}\")\n",
    "        pseudo_ds = Dataset.from_dict({\"text\": pseudo_texts, \"category\": pseudo_labels})\n",
    "        train_set = concatenate_datasets([train_set, pseudo_ds])\n",
    "        train_set = train_set.shuffle(seed=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Псевдолейблинг для классов. Главная проблема этого датасета - сильный дисбаланс классов. В частности, довольно маленький класс entertainment, кроме того в нём ещё и довольно плохие и \"жидкие\" примеры. По сути аугментации, псевдолейблинг и тп используются главным образом чтобы побороть именно этот один класс.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List categories:\n",
      "0 entertainment\n",
      "1 geography\n",
      "2 health\n",
      "3 politics\n",
      "4 science/technology\n",
      "5 sports\n",
      "6 travel\n"
     ]
    }
   ],
   "source": [
    "list_of_categories = sorted(\n",
    "    list(\n",
    "        set(train_set[\"category\"])\n",
    "        | set(validation_set[\"category\"])\n",
    "        | set(test_set[\"category\"])\n",
    "    )\n",
    ")\n",
    "indices_of_categories = list(range(len(list_of_categories)))\n",
    "n_categories = len(list_of_categories)\n",
    "id2label = dict(zip(indices_of_categories, list_of_categories))\n",
    "label2id = dict(zip(list_of_categories, indices_of_categories))\n",
    "\n",
    "print(\"\\nList categories:\")\n",
    "for i, c in enumerate(list_of_categories):\n",
    "    print(i, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Веса классов:\n",
      "  idx=0, label=entertainment, count=167, weight=5.1330\n",
      "  idx=1, label=geography, count=167, weight=1.5399\n",
      "  idx=2, label=health, count=77, weight=1.5119\n",
      "  idx=3, label=politics, count=102, weight=1.3136\n",
      "  idx=4, label=science/technology, count=176, weight=1.0000\n",
      "  idx=5, label=sports, count=85, weight=1.4390\n",
      "  idx=6, label=travel, count=138, weight=1.1293\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights(labels, label2id, method=\"sqrt\"):\n",
    "    counts_local = collections.Counter(labels)\n",
    "    classes_in_order = [lbl for lbl, _ in sorted(label2id.items(), key=lambda x: x[1])]\n",
    "    n_classes = len(classes_in_order)\n",
    "\n",
    "    class_counts_by_idx = [counts_local.get(lbl, 0) for lbl in classes_in_order]\n",
    "\n",
    "    if method == \"balanced\":\n",
    "        total = sum(class_counts_by_idx)\n",
    "        weights = [\n",
    "            (total / (n_classes * c)) if c > 0 else 1.0 for c in class_counts_by_idx\n",
    "        ]\n",
    "    elif method == \"inverse\":\n",
    "        max_c = max(class_counts_by_idx) if any(class_counts_by_idx) else 1\n",
    "        weights = [(max_c / c) if c > 0 else 1.0 for c in class_counts_by_idx]\n",
    "    elif method == \"sqrt\":\n",
    "        max_c = max(class_counts_by_idx) if any(class_counts_by_idx) else 1\n",
    "        weights = [np.sqrt(max_c / c) if c > 0 else 1.0 for c in class_counts_by_idx]\n",
    "    elif method == \"custom\":\n",
    "        max_c = max(class_counts_by_idx) if any(class_counts_by_idx) else 1\n",
    "        weights = [np.sqrt(max_c / c) if c > 0 else 1.0 for c in class_counts_by_idx]\n",
    "        for i, lbl in enumerate(classes_in_order):\n",
    "            if lbl == \"entertainment\":\n",
    "                weights[i] *= 5.0\n",
    "            if lbl == \"geography\":\n",
    "                weights[i] *= 1.5\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    print(\"\\nВеса классов:\")\n",
    "    for i, lbl in enumerate(classes_in_order):\n",
    "        print(\n",
    "            f\"  idx={i}, label={lbl}, count={class_counts_by_idx[i]}, weight={weights[i]:.4f}\"\n",
    "        )\n",
    "\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class_weights = compute_class_weights(train_set[\"category\"], label2id, method=\"custom\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за дисбаланса классов сделал взвешенное обучение, но никакие методы кроме агрессивного custom для самого проблемного класса особо не помогли. Во-первыз аугментации уже выравнивают количество примеров в обучающей выборке, во-вторых - проблем с перекрытием между классами лёгкие подправки весов не меняют ситуацию. 5.0 - подобрано ручками, и похоже является оптимумом для данного класса и датасета.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdb3a087a3040dd8df1ad213bf87313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/912 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3318a8d00f7f480ca6697b69f3df7162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tok(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "\n",
    "tokenized_train_set = train_set.map(tok, batched=True)\n",
    "tokenized_validation_set = validation_set.map(tok, batched=True)\n",
    "\n",
    "labeled_train_set = tokenized_train_set.add_column(\n",
    "    \"label\", [label2id[val] for val in tokenized_train_set[\"category\"]]\n",
    ")\n",
    "labeled_validation_set = tokenized_validation_set.add_column(\n",
    "    \"label\", [label2id[val] for val in tokenized_validation_set[\"category\"]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_metric = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    f1_macro = cls_metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"macro\"\n",
    "    )[\"f1\"]\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"f1\": f1_macro, \"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=n_categories,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    classifier_dropout=0.2,\n",
    "    hidden_dropout_prob=0.2,\n",
    "    attention_probs_dropout_prob=0.2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавлен дропаут для улучшения обучаемости, хотя реальных изменений в данном случае не обнаружено.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"rubert_sib200_weighted_v2\",\n",
    "    learning_rate=2.5e-5,\n",
    "    per_device_train_batch_size=MINIBATCH_SIZE,\n",
    "    per_device_eval_batch_size=MINIBATCH_SIZE,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.05,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    warmup_ratio=0.12,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    report_to=[\"none\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    label_smoothing_factor=0.03,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобраны оптимальные гиперпараметры. Сложно оценить как именно они повлияли и почему, но во многом всё упирается в размер датасета и то, что модель сходится довольно быстро.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33803/2873059226.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "Class weights: tensor([5.1330, 1.5399, 1.5119, 1.3136, 1.0000, 1.4390, 1.1293])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 01:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.693100</td>\n",
       "      <td>2.068080</td>\n",
       "      <td>0.077662</td>\n",
       "      <td>0.151515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.325800</td>\n",
       "      <td>0.590890</td>\n",
       "      <td>0.846934</td>\n",
       "      <td>0.858586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.349400</td>\n",
       "      <td>0.434496</td>\n",
       "      <td>0.838882</td>\n",
       "      <td>0.838384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.655787</td>\n",
       "      <td>0.850209</td>\n",
       "      <td>0.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.793864</td>\n",
       "      <td>0.842218</td>\n",
       "      <td>0.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.696421</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.868687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.760679</td>\n",
       "      <td>0.860219</td>\n",
       "      <td>0.858586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.802027</td>\n",
       "      <td>0.855349</td>\n",
       "      <td>0.858586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.791048</td>\n",
       "      <td>0.852570</td>\n",
       "      <td>0.858586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.793358</td>\n",
       "      <td>0.852570</td>\n",
       "      <td>0.858586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=570, training_loss=0.40600183783262445, metrics={'train_runtime': 76.278, 'train_samples_per_second': 119.563, 'train_steps_per_second': 7.473, 'total_flos': 218063076192000.0, 'train_loss': 0.40600183783262445, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(\n",
    "        self, model, inputs, return_outputs=False, num_items_in_batch=None\n",
    "    ):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        gamma = 1.5\n",
    "\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        labels_flat = labels.view(-1)\n",
    "\n",
    "        log_probs = nn.functional.log_softmax(logits_flat, dim=-1)\n",
    "        probs = log_probs.exp()\n",
    "\n",
    "        labels_flat_long = labels_flat.long()\n",
    "        idx = torch.arange(labels_flat_long.size(0), device=logits.device)\n",
    "        log_p_t = log_probs[idx, labels_flat_long]\n",
    "        p_t = probs[idx, labels_flat_long]\n",
    "\n",
    "        class_weights = self.class_weights.to(logits.device)\n",
    "        alpha_t = class_weights[labels_flat_long]\n",
    "\n",
    "        focal_factor = (1.0 - p_t) ** gamma\n",
    "        loss = -alpha_t * focal_factor * log_p_t\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    class_weights=class_weights,\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=labeled_train_set,\n",
    "    eval_dataset=labeled_validation_set,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Using CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation results:\n",
      "  eval_loss: 0.6964\n",
      "  eval_f1: 0.8640\n",
      "  eval_accuracy: 0.8687\n",
      "  eval_runtime: 0.1326\n",
      "  eval_samples_per_second: 746.8070\n",
      "  eval_steps_per_second: 98.0660\n",
      "  epoch: 10.0000\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(\"\\nValidation results:\")\n",
    "for key, value in results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "best_model = trainer.model\n",
    "\n",
    "\n",
    "def create_classification_pipeline_with_normalization(model, tokenizer, device=-1):\n",
    "    clf = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    def predict_with_normalization(texts):\n",
    "        normalized_texts = normalize_batch_texts(texts)\n",
    "        return clf(normalized_texts, truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "    return predict_with_normalization\n",
    "\n",
    "\n",
    "clf_normalized = create_classification_pipeline_with_normalization(\n",
    "    model=best_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Validation report:\n",
      "==================================================\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     entertainment     1.0000    0.7778    0.8750         9\n",
      "         geography     0.7500    0.7500    0.7500         8\n",
      "            health     1.0000    0.7273    0.8421        11\n",
      "          politics     0.9286    0.9286    0.9286        14\n",
      "science/technology     0.8889    0.9600    0.9231        25\n",
      "            sports     1.0000    0.9167    0.9565        12\n",
      "            travel     0.7083    0.8500    0.7727        20\n",
      "\n",
      "          accuracy                         0.8687        99\n",
      "         macro avg     0.8965    0.8443    0.8640        99\n",
      "      weighted avg     0.8827    0.8687    0.8702        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_texts = list(validation_set[\"text\"])\n",
    "pred_val = [label2id[x[\"label\"]] for x in clf_normalized(validation_texts)]\n",
    "true_val = [label2id[val] for val in validation_set[\"category\"]]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Validation report:\")\n",
    "print(\"=\" * 50)\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true=true_val, y_pred=pred_val, target_names=list_of_categories, digits=4\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Test report:\n",
      "==================================================\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     entertainment     0.8667    0.6842    0.7647        19\n",
      "         geography     0.8889    0.9412    0.9143        17\n",
      "            health     0.9130    0.9545    0.9333        22\n",
      "          politics     1.0000    0.9000    0.9474        30\n",
      "science/technology     0.8909    0.9608    0.9245        51\n",
      "            sports     0.9231    0.9600    0.9412        25\n",
      "            travel     0.9500    0.9500    0.9500        40\n",
      "\n",
      "          accuracy                         0.9216       204\n",
      "         macro avg     0.9189    0.9072    0.9108       204\n",
      "      weighted avg     0.9224    0.9216    0.9201       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_texts = list(test_set[\"text\"])\n",
    "pred_test = [label2id[x[\"label\"]] for x in clf_normalized(test_texts)]\n",
    "true_test = [label2id[val] for val in test_set[\"category\"]]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test report:\")\n",
    "print(\"=\" * 50)\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true=true_test, y_pred=pred_test, target_names=list_of_categories, digits=4\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалось немного побить бэйзлайн. Ключевые изменения - веса, именно агрессивное навязывание большого веса плохому классу помогло выиграть ~2% к f1 модели. Остальные изменения скорее стабилизировали обучение и внесли менее значимые изменения именно в метрику.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Трэшхолды\n",
    "\n",
    "Попробовал изменить трэшхолды классификации, чтобы ещё немного поднять метрику. Результаты ниже.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta=0.000 => macro F1 (val) = 0.8640\n",
      "delta=0.030 => macro F1 (val) = 0.8640\n",
      "delta=0.050 => macro F1 (val) = 0.8640\n",
      "delta=0.070 => macro F1 (val) = 0.8526\n",
      "delta=0.100 => macro F1 (val) = 0.8526\n",
      "delta=0.150 => macro F1 (val) = 0.8526\n",
      "delta=0.200 => macro F1 (val) = 0.8526\n",
      "delta=0.300 => macro F1 (val) = 0.8526\n",
      "delta=0.350 => macro F1 (val) = 0.8526\n",
      "delta=0.400 => macro F1 (val) = 0.8526\n",
      "delta=0.450 => macro F1 (val) = 0.8526\n",
      "\n",
      "best_delta=0.000; macro F1 on valid=0.8640\n"
     ]
    }
   ],
   "source": [
    "def get_logits_and_labels(dataset, model, data_collator, batch_size=MINIBATCH_SIZE):\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    torch_dataset = dataset.with_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"label\"],\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        torch_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_logits, all_labels\n",
    "\n",
    "\n",
    "val_logits, val_labels = get_logits_and_labels(\n",
    "    labeled_validation_set, trainer.model, data_collator\n",
    ")\n",
    "\n",
    "val_probs = F.softmax(val_logits, dim=-1).numpy()\n",
    "val_true = val_labels.numpy()\n",
    "\n",
    "ent_idx = label2id[\"entertainment\"]\n",
    "\n",
    "\n",
    "def predict_with_delta_entertainment(probs, delta):\n",
    "    preds = []\n",
    "    for p in probs:\n",
    "        top1_idx = int(p.argmax())\n",
    "        top1_prob = float(p[top1_idx])\n",
    "        ent_prob = float(p[ent_idx])\n",
    "\n",
    "        if top1_idx == ent_idx:\n",
    "            preds.append(ent_idx)\n",
    "            continue\n",
    "\n",
    "        if (top1_prob - ent_prob) <= delta and ent_prob >= 0.20:\n",
    "            preds.append(ent_idx)\n",
    "        else:\n",
    "            preds.append(top1_idx)\n",
    "    return np.array(preds, dtype=np.int64)\n",
    "\n",
    "\n",
    "candidate_deltas = [0.0, 0.03, 0.05, 0.07, 0.1, 0.15, 0.2, 0.3, 0.35, 0.4, 0.45]\n",
    "best_delta = 0.0\n",
    "best_f1 = -1.0\n",
    "\n",
    "for d in candidate_deltas:\n",
    "    preds_d = predict_with_delta_entertainment(val_probs, d)\n",
    "    f1_macro_d = f1_score(val_true, preds_d, average=\"macro\")\n",
    "    print(f\"delta={d:.3f} => macro F1 (val) = {f1_macro_d:.4f}\")\n",
    "    if f1_macro_d > best_f1:\n",
    "        best_f1 = f1_macro_d\n",
    "        best_delta = d\n",
    "\n",
    "print(f\"\\nbest_delta={best_delta:.3f}; macro F1 on valid={best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Validation report (with entertainment threshold tuning):\n",
      "==================================================\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     entertainment     1.0000    0.7778    0.8750         9\n",
      "         geography     0.7500    0.7500    0.7500         8\n",
      "            health     1.0000    0.7273    0.8421        11\n",
      "          politics     0.9286    0.9286    0.9286        14\n",
      "science/technology     0.8889    0.9600    0.9231        25\n",
      "            sports     1.0000    0.9167    0.9565        12\n",
      "            travel     0.7083    0.8500    0.7727        20\n",
      "\n",
      "          accuracy                         0.8687        99\n",
      "         macro avg     0.8965    0.8443    0.8640        99\n",
      "      weighted avg     0.8827    0.8687    0.8702        99\n",
      "\n",
      "\n",
      "==================================================\n",
      "Test report (with entertainment threshold tuning):\n",
      "==================================================\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     entertainment     0.8667    0.6842    0.7647        19\n",
      "         geography     0.8889    0.9412    0.9143        17\n",
      "            health     0.9130    0.9545    0.9333        22\n",
      "          politics     1.0000    0.9000    0.9474        30\n",
      "science/technology     0.8909    0.9608    0.9245        51\n",
      "            sports     0.9231    0.9600    0.9412        25\n",
      "            travel     0.9500    0.9500    0.9500        40\n",
      "\n",
      "          accuracy                         0.9216       204\n",
      "         macro avg     0.9189    0.9072    0.9108       204\n",
      "      weighted avg     0.9224    0.9216    0.9201       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_preds_threshold = predict_with_delta_entertainment(val_probs, best_delta)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Validation report (with entertainment threshold tuning):\")\n",
    "print(\"=\" * 50)\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true=val_true,\n",
    "        y_pred=val_preds_threshold,\n",
    "        target_names=list_of_categories,\n",
    "        digits=4,\n",
    "    )\n",
    ")\n",
    "\n",
    "tokenized_test_set = test_set.map(tok, batched=True)\n",
    "labeled_test_set = tokenized_test_set.add_column(\n",
    "    \"label\", [label2id[val] for val in tokenized_test_set[\"category\"]]\n",
    ")\n",
    "\n",
    "test_logits, test_labels = get_logits_and_labels(\n",
    "    labeled_test_set, trainer.model, data_collator\n",
    ")\n",
    "test_probs = F.softmax(test_logits, dim=-1).numpy()\n",
    "test_true = test_labels.numpy()\n",
    "\n",
    "test_preds_threshold = predict_with_delta_entertainment(test_probs, best_delta)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test report (with entertainment threshold tuning):\")\n",
    "print(\"=\" * 50)\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true=test_true,\n",
    "        y_pred=test_preds_threshold,\n",
    "        target_names=list_of_categories,\n",
    "        digits=4,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, трэшхолды не влияют на улучшение метрики, значит скорее всего эта модель уже оптимум. В такой ситуации скорее всего может помось либо использования более крупной модели (хотел попробовать, но не влезло в VRAM), либо порабоать с классом entertainment, добавив новых примеров и уменьшив его схожесть с другими классами.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-kernel",
   "language": "python",
   "name": "nlp-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
