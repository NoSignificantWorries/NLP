{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download ru_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import spacy\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета SIB-200 (русский, кириллица) и подготовка меток\n",
    "# Возвращает кортежи (тексты, метки) для train/val/test и список названий классов в фиксированном порядке\n",
    "\n",
    "def load_sib200_ru() -> Tuple[Tuple[List[str], List[int]], Tuple[List[str], List[int]], Tuple[List[str], List[int]], List[str]]:\n",
    "    trainset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='train')\n",
    "    X_train = trainset['text']\n",
    "    y_train = trainset['category']\n",
    "    valset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='validation')\n",
    "    X_val = valset['text']\n",
    "    y_val = valset['category']\n",
    "    testset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='test')\n",
    "    X_test = testset['text']\n",
    "    y_test = testset['category']\n",
    "\n",
    "    # Проверяем, что во валидации/тесте нет новых классов, отсутствующих в трейне\n",
    "    categories = set(y_train)\n",
    "    unknown_categories = set(y_val) - categories\n",
    "    if len(unknown_categories) > 0:\n",
    "        err_msg = f'The categories {unknown_categories} are represented in the validation set, but they are not represented in the training set.'\n",
    "        raise RuntimeError(err_msg)\n",
    "    unknown_categories = set(y_test) - categories\n",
    "    if len(unknown_categories) > 0:\n",
    "        err_msg = f'The categories {unknown_categories} are represented in the test set, but they are not represented in the training set.'\n",
    "        raise RuntimeError(err_msg)\n",
    "\n",
    "    # Фиксируем порядок классов и переводим строковые метки в индексы\n",
    "    categories = sorted(list(categories))\n",
    "    y_train = [categories.index(it) for it in y_train]\n",
    "    y_val = [categories.index(it) for it in y_val]\n",
    "    y_test = [categories.index(it) for it in y_test]\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация текста: токенизация spaCy, лемматизация, замена чисел на <NUM>, удаление пунктуации\n",
    "\n",
    "def normalize_text(s: str, nlp_pipeline: spacy.Language) -> str:\n",
    "    doc = nlp_pipeline(s)\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space:\n",
    "            continue\n",
    "        if token.like_num:\n",
    "            lemmas.append('<NUM>')\n",
    "        elif token.is_stop:  # Удаляем стоп-слова\n",
    "            continue\n",
    "        elif token.lemma_.strip():  # Проверяем, что лемма не пустая\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join(lemmas) if lemmas else ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Режим ускорения: если True — сильно сокращаем сетку и число фолдов, чтобы обучаться быстрее\n",
    "FAST_MODE = True\n",
    "\n",
    "train_data, val_data, test_data, classes_list = load_sib200_ru()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Categories: {classes_list}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_data[0]))\n",
    "# print(len(train_data[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(val_data[0]))\n",
    "# print(len(val_data[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(test_data[0]))\n",
    "# print(len(test_data[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем модель spaCy для русского. Для скорости можно отключить лишние компоненты,\n",
    "# но в ru_core_news_sm лемматизация зависит от теггера; оставим по умолчанию.\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "\n",
    "# Предварительная нормализация корпусов для train/val/test\n",
    "train_norm = [normalize_text(t, nlp) for t in train_data[0]]\n",
    "val_norm = [normalize_text(t, nlp) for t in val_data[0]]\n",
    "test_norm = [normalize_text(t, nlp) for t in test_data[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(normalize_text(train_data[0][0], nlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(val_data[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(normalize_text(val_data[0][0], nlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_data[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(normalize_text(test_data[0][0], nlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_df вычислялся как 1 - 0.2 * p(class), что практически равно ~0.999 при 200 классах\n",
    "# и почти не фильтрует частые токены. Оставим расчёт, но дальше дадим сетке перебрать разумные значения.\n",
    "class_probability = 1.0 / len(classes_list)\n",
    "max_df = 1.0 - 0.2 * class_probability\n",
    "# print(f'Maximal document frequency of term is {max_df}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# БАЗОВЫЙ ВАРИАНТ (оставляем закомментированным для сравнения):\n",
    "# classifier = Pipeline(steps=[\n",
    "#     ('vectorizer', TfidfVectorizer(token_pattern='\\\\w+', max_df=max_df, min_df=1)),\n",
    "#     ('cls', LogisticRegression(solver='saga', max_iter=100, random_state=42))\n",
    "# ])\n",
    "\n",
    "# УЛУЧШЕННЫЙ ВАРИАНТ:\n",
    "# 1) Переносим нормализацию внутрь пайплайна че��ез FunctionTransformer, чтобы избежать утечек при CV.\n",
    "# 2) Добавляем признаковое объединение: word TF-IDF + char n-gram TF-IDF (char_wb), что часто повышает F1 на русском.\n",
    "# 3) Включаем sublinear_tf и настраиваем min_df/max_df.\n",
    "# 4) Увеличиваем max_iter и добавляем опции class_weight/multi_class для LR через сетку.\n",
    "\n",
    "# preprocess больше не используется внутри Pipeline из-за проблем совместимости и производительности.\n",
    "# preprocess = FunctionTransformer(_normalize_batch, validate=False)\n",
    "\n",
    "# Две ветки признаков: слова и символы\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    token_pattern=r'(?u)\\b\\w+\\b',\n",
    "    sublinear_tf=True,\n",
    "    max_df=max_df,\n",
    "    min_df=1,\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char_wb',\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(2, 6),\n",
    "    min_df=1\n",
    ")\n",
    "feature_union = FeatureUnion([\n",
    "    ('word', word_vectorizer),\n",
    "    ('char', char_vectorizer)\n",
    "])\n",
    "\n",
    "# Убираем кэширование шагов пайплайна (Memory) из-за проблем �� сериализацией FunctionTransformer в некоторых окружениях\n",
    "# Модель по-прежнему сохраняется на диск после обучения.\n",
    "# Ранее нормализация была шагом пайплайна:\n",
    "# classifier = Pipeline(steps=[\n",
    "#     ('preprocess', preprocess),\n",
    "#     ('features', feature_union),\n",
    "#     ('cls', LogisticRegression(solver='saga', max_iter=2000, random_state=42))\n",
    "# ])\n",
    "# Теперь используем пайплайн без шага preprocess. Нормализуем тексты заранее (см. fit/predict ниже).\n",
    "classifier = Pipeline(steps=[\n",
    "    ('features', feature_union),\n",
    "    ('cls', MultinomialNB())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_params = {'cls__alpha': [0.025],\n",
    "               'cls__fit_prior': [False],\n",
    "               'features__char__ngram_range': [(3, 5)],\n",
    "               'features__word__max_df': [0.9],\n",
    "               'features__word__min_df': [1],\n",
    "               'features__word__ngram_range': [(1, 1)]}\n",
    "\n",
    "# БАЗОВАЯ СЕТКА (оставлена для справки):\n",
    "# cv = GridSearchCV(\n",
    "#     estimator=classifier,\n",
    "#     param_grid={\n",
    "#         'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "#         'cls__C': [1e-1, 1, 10, 100, 1000],\n",
    "#         'cls__penalty': ['l1', 'l2']\n",
    "#     },\n",
    "#     scoring='f1_macro',\n",
    "#     cv=5,\n",
    "#     refit=True,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# НОВАЯ СЕТКА ДЛЯ УЛУЧШЕННОГО ПАЙПЛАЙНА:\n",
    "# - Тюним n-gram для слов, min_df/max_df, а также силу регуляризации LR и class_weight/multi_class.\n",
    "# Сформируем сетку и параметры CV в зависимости от FAST_MODE\n",
    "if FAST_MODE:\n",
    "    # Ускоренный режим: меньше комбинаций, меньше фолдов\n",
    "    param_grid = {\n",
    "        'features__word__ngram_range': [(1, 2), (1, 3)],\n",
    "        'features__word__min_df': [1, 2],\n",
    "        'features__word__max_df': [0.95, 0.98, 1.0],\n",
    "        'features__char__ngram_range': [(3, 6), (2, 6)],\n",
    "        'cls__alpha': [0.05, 0.1, 0.25, 0.5, 1.0],\n",
    "        'cls__fit_prior': [True, False],\n",
    "    }\n",
    "    cv_folds = 3\n",
    "    verbose_level = 3  # подробный прогресс\n",
    "else:\n",
    "    param_grid = {\n",
    "        'features__word__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'features__word__min_df': [1, 2, 3, 5],\n",
    "        'features__word__max_df': [0.9, 0.95, 0.98, 1.0],\n",
    "        'features__char__ngram_range': [(3, 5), (3, 6), (2, 6)],\n",
    "        'cls__alpha': [0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.0],\n",
    "        'cls__fit_prior': [True, False],\n",
    "    }\n",
    "    cv_folds = 5\n",
    "    verbose_level = 2\n",
    "\n",
    "cv = GridSearchCV(\n",
    "    estimator=classifier,\n",
    "    param_grid=param_grid if best_params is None else best_params,\n",
    "    scoring='f1_macro',\n",
    "    cv=cv_folds,\n",
    "    refit=True,\n",
    "    n_jobs=-1,\n",
    "    verbose=verbose_level\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ранее нормализация считалась вне пайплайна (это могло приводить к несогласованности при CV):\n",
    "# cv.fit([normalize_text(it, nlp) for it in train_data[0]], train_data[1])\n",
    "# Теперь нормализация выполняется вне пайплайна один раз для всех текстов\n",
    "cv.fit(train_norm, train_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters:', cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best F1-macro:', cv.best_score_)\n",
    "\n",
    "# Сохраним лучшую модель и её параметры на диск, чтобы не переобучать в следующий раз\n",
    "# Убрано сохранение на диск по просьбе: не сохраняем модель и параметры\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размер словаря теперь является суммой по двум векторизаторам. Для демонстрации выведем размеры по веткам.\n",
    "best_est = cv.best_estimator_\n",
    "word_vocab_size = len(best_est.named_steps['features'].transformer_list[0][1].vocabulary_)\n",
    "char_vocab_size = len(best_est.named_steps['features'].transformer_list[1][1].vocabulary_)\n",
    "print(f'Word vocab size: {word_vocab_size}; Char vocab size: {char_vocab_size}; Total approx: {word_vocab_size + char_vocab_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание на валидации\n",
    "y_pred = cv.predict(val_norm)\n",
    "print(classification_report(y_true=val_data[1], y_pred=y_pred, target_names=classes_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание на тесте\n",
    "y_pred = cv.predict(test_norm)\n",
    "print(classification_report(y_true=test_data[1], y_pred=y_pred, target_names=classes_list))\n",
    "cv.fit(train_norm, train_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters:', cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best F1-macro:', cv.best_score_)\n",
    "\n",
    "# Сохраним лучшую модель и её параметры на диск, чтобы не переобучать в следующий раз\n",
    "# Убрано сохранение на диск по просьбе: не сохраняем модель и параметры\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размер словаря теперь является суммой по двум векторизаторам. Для демонстрации выведем размеры по веткам.\n",
    "best_est = cv.best_estimator_\n",
    "word_vocab_size = len(best_est.named_steps['features'].transformer_list[0][1].vocabulary_)\n",
    "char_vocab_size = len(best_est.named_steps['features'].transformer_list[1][1].vocabulary_)\n",
    "print(f'Word vocab size: {word_vocab_size}; Char vocab size: {char_vocab_size}; Total approx: {word_vocab_size + char_vocab_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание на валидации\n",
    "y_pred = cv.predict(val_norm)\n",
    "print(classification_report(y_true=val_data[1], y_pred=y_pred, target_names=classes_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание на тесте\n",
    "y_pred = cv.predict(test_norm)\n",
    "print(classification_report(y_true=test_data[1], y_pred=y_pred, target_names=classes_list))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
